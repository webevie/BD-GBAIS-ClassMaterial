{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:23:28 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:23:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Incident Management dataset has about 141712 records of 24918 incidents. Each state of the incident is being captured as an individual record with few exceptions where the closed state of an incident is recorded more than once. With the help of the below segment of the code, we load and clean the Incident Management data so that only one record representing the truly closed state per incident is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:23:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|    number|incident_state|active|reassignment_count|reopen_count|sys_mod_count|made_sla|  caller_id|     opened_by|      opened_at|sys_created_by| sys_created_at|sys_updated_by|sys_updated_at|contact_type|    location|   category|    subcategory|  u_symptom|cmdb_ci|    impact|   urgency|    priority|assignment_group| assigned_to|knowledge|u_priority_confirmation|       notify|   problem_id|rfc|vendor|caused_by|closed_code|    resolved_by|    resolved_at|     closed_at|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|INC0000045|        Closed| false|                 0|           0|            4|    true|Caller 2403|  Opened by  8|29/2/2016 01:16|  Created by 6|29/2/2016 01:23|Updated by 908|5/3/2016 12:00|       Phone|Location 143|Category 55|Subcategory 170| Symptom 72|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 56|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5|Resolved by 149|29/2/2016 11:29|5/3/2016 12:00|\n",
      "|INC0000047|        Closed| false|                 1|           0|            8|    true|Caller 2403|Opened by  397|29/2/2016 04:40|Created by 171|29/2/2016 04:57|Updated by 908|6/3/2016 10:00|       Phone|Location 165|Category 40|Subcategory 215|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 24| Resolver 89|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5| Resolved by 81|  1/3/2016 9:52|6/3/2016 10:00|\n",
      "|INC0000057|        Closed| false|                 0|           0|            6|    true|Caller 4416|  Opened by  8|29/2/2016 06:10|             ?|              ?|Updated by 908| 6/3/2016 3:00|       Phone|Location 204|Category 20|Subcategory 125|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 70|  Resolver 6|     true|                  false|Do Not Notify|Problem ID  2|  ?|     ?|        ?|    code 10|  Resolved by 5|  1/3/2016 2:55| 6/3/2016 3:00|\n",
      "|INC0000060|        Closed| false|                 0|           0|            3|    true|Caller 4491|Opened by  180|29/2/2016 06:38| Created by 81|29/2/2016 06:42|Updated by 908|7/3/2016 13:00|       Phone|Location 204| Category 9| Subcategory 97|Symptom 450|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 25|Resolver 125|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 3|Resolved by 113| 2/3/2016 12:06|7/3/2016 13:00|\n",
      "|INC0000062|        Closed| false|                 1|           0|            7|   false|Caller 3765|Opened by  180|29/2/2016 06:58| Created by 81|29/2/2016 07:26|Updated by 908|5/3/2016 16:00|       Phone| Location 93|Category 53|Subcategory 168|Symptom 232|      ?|  1 - High|2 - Medium|    2 - High|        Group 23|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 7| Resolved by 62|29/2/2016 15:51|5/3/2016 16:00|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "df = spark.read.csv('data/incident_event_log_reduced.csv', header=True, inferSchema=True)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has multiple states(New, Active, Awaiting user info, Resolved, Closed etc. ) of an incident. With the help of the below command, we are just filtering one record per incident, that has the truly closed state of the incident. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_incidents=df.filter(\"incident_state=='Closed'\").sort(\"sys_mod_count\",ascending=False).dropDuplicates([\"number\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the dependent and the independent variables that are identified as most useful attributes to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_unique_incidents.select([\n",
    "    'caller_id',\n",
    "    'opened_by',\n",
    "    'location',\n",
    "    'category',\n",
    "    'subcategory',\n",
    "    'u_symptom',\n",
    "    'assignment_group',\n",
    "    'priority'\n",
    "    ]\n",
    ")\n",
    "data=data.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 70-30 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use StringIndexer to convert the categorical columns to hold numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_id_indexer = StringIndexer(inputCol='caller_id',outputCol='caller_id_index',handleInvalid='keep')\n",
    "opened_by_indexer = StringIndexer(inputCol='opened_by',outputCol='opened_by_index',handleInvalid='keep')\n",
    "location_indexer = StringIndexer(inputCol='location',outputCol='location_index',handleInvalid='keep')\n",
    "category_indexer = StringIndexer(inputCol='category',outputCol='category_index',handleInvalid='keep')\n",
    "subcategory_indexer = StringIndexer(inputCol='subcategory',outputCol='subcategory_index',handleInvalid='keep')\n",
    "u_symptom_indexer = StringIndexer(inputCol='u_symptom',outputCol='u_symptom_index',handleInvalid='keep')\n",
    "assignment_group_indexer = StringIndexer(inputCol='assignment_group',outputCol='assignment_group_index',handleInvalid='keep')\n",
    "priority_indexer = StringIndexer(inputCol='priority',outputCol='priority_index',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector assembler is used to create a vector of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'caller_id_index',\n",
    "        'opened_by_index',\n",
    "        'location_index',\n",
    "        'category_index',\n",
    "        'subcategory_index',\n",
    "        'u_symptom_index',\n",
    "        'assignment_group_index'\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object for the Logistic Regression model. Use the parameter maxBins and assign a value that is equal to or more than the number of categories in any sigle feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(labelCol='priority_index',maxBins=5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data in the same way as that of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        caller_id_indexer,\n",
    "        opened_by_indexer,\n",
    "        location_indexer,\n",
    "        category_indexer,\n",
    "        subcategory_indexer,\n",
    "        u_symptom_indexer,\n",
    "        assignment_group_indexer,\n",
    "        priority_indexer,\n",
    "        assembler,\n",
    "        dt_model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fit_model=pipe.fit(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|priority_index|prediction|\n",
      "+--------------+----------+\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           1.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['priority_index','prediction']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the decision tree classifier is 0.9429530201342282\n"
     ]
    }
   ],
   "source": [
    "ACC_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"priority_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = ACC_evaluator.evaluate(results)\n",
    "\n",
    "print(f\"The accuracy of the decision tree classifier is {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you wish to look at other model evaluation metrics, see previous notebooks in this series for examples of f1_score, precision, recall, areaUnderROC, and areaUnderPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
