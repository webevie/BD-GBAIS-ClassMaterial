{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:16:47 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:16:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:16:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 PySpark Tutorials</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4d38088890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:16:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/26 15:16:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/26 15:16:54 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/26 15:16:54 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore student@127.0.0.1\n",
      "23/10/26 15:16:54 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|fake_friends|      false|\n",
      "|  default|   incidents|      false|\n",
      "|  default|movieratings|      false|\n",
      "|  default|      movies|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT movieid)|\n",
      "+-----------------------+\n",
      "|                   1682|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select count(DISTINCT movieid) from movieratings\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  100000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select count(*) from movieratings\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|count|rating|\n",
      "+-----+------+\n",
      "|  583| 4.358|\n",
      "|  509| 3.804|\n",
      "|  508| 4.156|\n",
      "|  507| 4.008|\n",
      "|  485| 3.157|\n",
      "|  481| 3.657|\n",
      "|  478| 3.441|\n",
      "|  452| 3.878|\n",
      "|  431| 3.631|\n",
      "|  429| 3.438|\n",
      "|  420| 4.252|\n",
      "|  413| 4.283|\n",
      "|  394| 4.061|\n",
      "|  392| 3.798|\n",
      "|  390|  4.29|\n",
      "|  384| 3.711|\n",
      "|  378| 3.693|\n",
      "|  367| 4.204|\n",
      "|  365|  3.66|\n",
      "|  350| 4.246|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRatingCount = spark.sql(\"select count(movieid) as count, round(avg(rating),3) as rating  \\\n",
    "                    from movieratings group by movieid order by count desc, rating desc\")\n",
    "dfRatingCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "|count|rating|features|\n",
      "+-----+------+--------+\n",
      "|  583| 4.358| [583.0]|\n",
      "|  509| 3.804| [509.0]|\n",
      "|  508| 4.156| [508.0]|\n",
      "|  507| 4.008| [507.0]|\n",
      "|  485| 3.157| [485.0]|\n",
      "|  481| 3.657| [481.0]|\n",
      "|  478| 3.441| [478.0]|\n",
      "|  452| 3.878| [452.0]|\n",
      "|  431| 3.631| [431.0]|\n",
      "|  429| 3.438| [429.0]|\n",
      "|  420| 4.252| [420.0]|\n",
      "|  413| 4.283| [413.0]|\n",
      "|  394| 4.061| [394.0]|\n",
      "|  392| 3.798| [392.0]|\n",
      "|  390|  4.29| [390.0]|\n",
      "|  384| 3.711| [384.0]|\n",
      "|  378| 3.693| [378.0]|\n",
      "|  367| 4.204| [367.0]|\n",
      "|  365|  3.66| [365.0]|\n",
      "|  350| 4.246| [350.0]|\n",
      "+-----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    " \n",
    "# For more on VectorAssembler, see https://spark.apache.org/docs/latest/ml-features.html#vectorassembler \n",
    "dfAssemblerFeature =  VectorAssembler(\n",
    "    inputCols=[\"count\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    " \n",
    "dfRatingCount = dfAssemblerFeature.transform(dfRatingCount)\n",
    "dfRatingCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|features|rating|\n",
      "+--------+------+\n",
      "| [583.0]| 4.358|\n",
      "| [509.0]| 3.804|\n",
      "| [508.0]| 4.156|\n",
      "| [507.0]| 4.008|\n",
      "| [485.0]| 3.157|\n",
      "| [481.0]| 3.657|\n",
      "| [478.0]| 3.441|\n",
      "| [452.0]| 3.878|\n",
      "| [431.0]| 3.631|\n",
      "| [429.0]| 3.438|\n",
      "| [420.0]| 4.252|\n",
      "| [413.0]| 4.283|\n",
      "| [394.0]| 4.061|\n",
      "| [392.0]| 3.798|\n",
      "| [390.0]|  4.29|\n",
      "| [384.0]| 3.711|\n",
      "| [378.0]| 3.693|\n",
      "| [367.0]| 4.204|\n",
      "| [365.0]|  3.66|\n",
      "| [350.0]| 4.246|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRatingCount = dfRatingCount.select(\"features\", \"rating\")\n",
    "dfRatingCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.0]\n",
      "degreesOfFreedom: [213248]\n",
      "statistics: [339830.8603909695]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "r = ChiSquareTest.test(dfRatingCount, \"features\", \"rating\").head()\n",
    " \n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:17:04 WARN Instrumentation: [13db7844] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: 0.00418\n",
      "Intercept: 2.82766\n"
     ]
    }
   ],
   "source": [
    "# For more information on LinearRegression, see https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\n",
    "lr = LinearRegression(maxIter=10, featuresCol=\"features\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "# Fit the model\n",
    "lrModel = lr.fit(dfRatingCount)\n",
    " \n",
    " \n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lrModel.coefficients[0]:.5f}\")\n",
    "print(f\"Intercept: {lrModel.intercept:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------------------+\n",
      "|features|rating|        prediction|\n",
      "+--------+------+------------------+\n",
      "| [583.0]| 4.358| 5.263360163655971|\n",
      "| [509.0]| 3.804|  4.95419766550137|\n",
      "| [508.0]| 4.156| 4.950019793904686|\n",
      "| [507.0]| 4.008| 4.945841922308002|\n",
      "| [485.0]| 3.157| 4.853928747180959|\n",
      "| [481.0]| 3.657| 4.837217260794223|\n",
      "| [478.0]| 3.441| 4.824683646004172|\n",
      "| [452.0]| 3.878|4.7160589844903935|\n",
      "| [431.0]| 3.631| 4.628323680960033|\n",
      "| [429.0]| 3.438| 4.619967937766665|\n",
      "| [420.0]| 4.252|4.5823670933965115|\n",
      "| [413.0]| 4.283| 4.553121992219724|\n",
      "| [394.0]| 4.061| 4.473742431882732|\n",
      "| [392.0]| 3.798| 4.465386688689365|\n",
      "| [390.0]|  4.29|4.4570309454959975|\n",
      "| [384.0]| 3.711| 4.431963715915894|\n",
      "| [378.0]| 3.693| 4.406896486335792|\n",
      "| [367.0]| 4.204| 4.360939898772269|\n",
      "| [365.0]|  3.66| 4.352584155578902|\n",
      "| [350.0]| 4.246| 4.289916081628645|\n",
      "| [350.0]| 3.834| 4.289916081628645|\n",
      "| [344.0]| 3.314| 4.264848852048543|\n",
      "| [336.0]| 4.045| 4.231425879275072|\n",
      "| [331.0]| 3.931| 4.210536521291653|\n",
      "| [326.0]| 3.632|4.1896471633082335|\n",
      "| [324.0]| 4.173| 4.181291420114866|\n",
      "| [321.0]| 3.854| 4.168757805324814|\n",
      "| [316.0]| 4.066| 4.147868447341396|\n",
      "| [316.0]| 3.123| 4.147868447341396|\n",
      "| [315.0]| 3.927| 4.143690575744712|\n",
      "| [303.0]| 3.746| 4.093556116584506|\n",
      "| [301.0]| 3.934| 4.085200373391139|\n",
      "| [300.0]| 3.833| 4.081022501794455|\n",
      "| [299.0]| 3.896| 4.076844630197771|\n",
      "| [298.0]| 4.466| 4.072666758601088|\n",
      "| [298.0]| 3.698| 4.072666758601088|\n",
      "| [297.0]| 4.162| 4.068488887004404|\n",
      "| [297.0]| 4.152| 4.068488887004404|\n",
      "| [295.0]| 4.007|4.0601331438110355|\n",
      "| [295.0]| 3.424|4.0601331438110355|\n",
      "| [293.0]| 3.778| 4.051777400617668|\n",
      "| [293.0]| 3.444| 4.051777400617668|\n",
      "| [293.0]| 3.215| 4.051777400617668|\n",
      "| [291.0]| 4.034|   4.0434216574243|\n",
      "| [290.0]|  3.91| 4.039243785827617|\n",
      "| [284.0]| 3.947| 4.014176556247514|\n",
      "| [283.0]| 4.445|  4.00999868465083|\n",
      "| [280.0]| 3.775|3.9974650698607785|\n",
      "| [280.0]| 3.764|3.9974650698607785|\n",
      "| [276.0]| 4.163|3.9807535834740433|\n",
      "| [276.0]| 3.931|3.9807535834740433|\n",
      "| [275.0]| 4.138|3.9765757118773593|\n",
      "| [272.0]| 3.485| 3.964042097087308|\n",
      "| [268.0]| 4.011| 3.947330610700573|\n",
      "| [267.0]| 4.386| 3.943152739103889|\n",
      "| [267.0]| 3.644| 3.943152739103889|\n",
      "| [264.0]| 4.292|3.9306191243138375|\n",
      "| [261.0]|  3.72|3.9180855095237863|\n",
      "| [259.0]| 3.969|3.9097297663304187|\n",
      "| [259.0]| 2.981|3.9097297663304187|\n",
      "| [256.0]| 3.875| 3.897196151540367|\n",
      "| [256.0]| 3.793| 3.897196151540367|\n",
      "| [255.0]| 3.792|3.8930182799436834|\n",
      "| [254.0]| 3.031|    3.888840408347|\n",
      "| [251.0]| 3.916| 3.876306793556948|\n",
      "| [251.0]| 3.837| 3.876306793556948|\n",
      "| [251.0]| 3.661| 3.876306793556948|\n",
      "| [251.0]| 3.594| 3.876306793556948|\n",
      "| [250.0]| 3.884|3.8721289219602646|\n",
      "| [247.0]| 3.785| 3.859595307170213|\n",
      "| [246.0]| 4.077|3.8554174355735293|\n",
      "| [244.0]| 3.816|3.8470616923801613|\n",
      "| [244.0]| 3.557|3.8470616923801613|\n",
      "| [243.0]| 4.457|3.8428838207834777|\n",
      "| [243.0]| 3.872|3.8428838207834777|\n",
      "| [241.0]| 4.058|  3.83452807759011|\n",
      "| [240.0]| 3.108|3.8303502059934265|\n",
      "| [240.0]| 2.933|3.8303502059934265|\n",
      "| [239.0]| 4.105|3.8261723343967424|\n",
      "| [239.0]|   4.1|3.8261723343967424|\n",
      "| [236.0]| 3.847| 3.813638719606691|\n",
      "| [232.0]| 3.685| 3.796927233219956|\n",
      "| [231.0]| 4.121|3.7927493616232724|\n",
      "| [230.0]| 3.648|3.7885714900265883|\n",
      "| [230.0]| 3.304|3.7885714900265883|\n",
      "| [227.0]| 3.881| 3.776037875236537|\n",
      "| [227.0]| 3.863| 3.776037875236537|\n",
      "| [226.0]| 3.951| 3.771860003639853|\n",
      "| [223.0]|  3.57| 3.759326388849802|\n",
      "| [222.0]| 3.766| 3.755148517253118|\n",
      "| [221.0]| 4.045| 3.750970645656434|\n",
      "| [221.0]|  3.91| 3.750970645656434|\n",
      "| [221.0]| 3.611| 3.750970645656434|\n",
      "| [220.0]| 3.782|  3.74679277405975|\n",
      "| [220.0]| 3.482|  3.74679277405975|\n",
      "| [219.0]| 4.292|3.7426149024630666|\n",
      "| [219.0]| 3.995|3.7426149024630666|\n",
      "| [219.0]| 3.813|3.7426149024630666|\n",
      "| [219.0]| 2.808|3.7426149024630666|\n",
      "| [218.0]| 3.087|3.7384370308663826|\n",
      "+--------+------+------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRatingCount = lrModel.transform(dfRatingCount)\n",
    "dfRatingCount.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7056\n",
      "r2: 0.1846\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(f\"RMSE: {trainingSummary.rootMeanSquaredError:.4f}\")\n",
    "print(f\"r2: {trainingSummary.r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
