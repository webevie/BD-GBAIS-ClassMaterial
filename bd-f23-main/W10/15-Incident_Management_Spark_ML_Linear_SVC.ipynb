{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:21:10 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:21:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:21:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC to predict whether an incident met SLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Incident Management dataset has about 141712 records of 24918 incidents. Each state of the incident is being captured as an individual record with few exceptions where the closed state of an incident is recorded more than once. With the help of the below segment of the code, we load and clean the Incident Management data so that only one record representing the truly closed state per incident is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:21:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|    number|incident_state|active|reassignment_count|reopen_count|sys_mod_count|made_sla|  caller_id|     opened_by|      opened_at|sys_created_by| sys_created_at|sys_updated_by|sys_updated_at|contact_type|    location|   category|    subcategory|  u_symptom|cmdb_ci|    impact|   urgency|    priority|assignment_group| assigned_to|knowledge|u_priority_confirmation|       notify|   problem_id|rfc|vendor|caused_by|closed_code|    resolved_by|    resolved_at|     closed_at|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|INC0000045|        Closed| false|                 0|           0|            4|    true|Caller 2403|  Opened by  8|29/2/2016 01:16|  Created by 6|29/2/2016 01:23|Updated by 908|5/3/2016 12:00|       Phone|Location 143|Category 55|Subcategory 170| Symptom 72|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 56|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5|Resolved by 149|29/2/2016 11:29|5/3/2016 12:00|\n",
      "|INC0000047|        Closed| false|                 1|           0|            8|    true|Caller 2403|Opened by  397|29/2/2016 04:40|Created by 171|29/2/2016 04:57|Updated by 908|6/3/2016 10:00|       Phone|Location 165|Category 40|Subcategory 215|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 24| Resolver 89|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5| Resolved by 81|  1/3/2016 9:52|6/3/2016 10:00|\n",
      "|INC0000057|        Closed| false|                 0|           0|            6|    true|Caller 4416|  Opened by  8|29/2/2016 06:10|             ?|              ?|Updated by 908| 6/3/2016 3:00|       Phone|Location 204|Category 20|Subcategory 125|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 70|  Resolver 6|     true|                  false|Do Not Notify|Problem ID  2|  ?|     ?|        ?|    code 10|  Resolved by 5|  1/3/2016 2:55| 6/3/2016 3:00|\n",
      "|INC0000060|        Closed| false|                 0|           0|            3|    true|Caller 4491|Opened by  180|29/2/2016 06:38| Created by 81|29/2/2016 06:42|Updated by 908|7/3/2016 13:00|       Phone|Location 204| Category 9| Subcategory 97|Symptom 450|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 25|Resolver 125|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 3|Resolved by 113| 2/3/2016 12:06|7/3/2016 13:00|\n",
      "|INC0000062|        Closed| false|                 1|           0|            7|   false|Caller 3765|Opened by  180|29/2/2016 06:58| Created by 81|29/2/2016 07:26|Updated by 908|5/3/2016 16:00|       Phone| Location 93|Category 53|Subcategory 168|Symptom 232|      ?|  1 - High|2 - Medium|    2 - High|        Group 23|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 7| Resolved by 62|29/2/2016 15:51|5/3/2016 16:00|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "df = spark.read.csv('data/incident_event_log_reduced.csv', header=True, inferSchema=True)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    " \n",
    "df=df.withColumn('resolved_ts',to_timestamp(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened_ts',to_timestamp(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed_ts',to_timestamp(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('resolved',to_date(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened',to_date(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed',to_date(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('knowledge', f.col('knowledge').cast('string')).\\\n",
    "        replace(['TRUE',], 'True', subset='knowledge').\\\n",
    "        replace(['FALSE'], 'False', subset='knowledge').\\\n",
    "        withColumn('resolved_duration',datediff(to_date(df.resolved_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('closed_duration',datediff(to_date(df.closed_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('made_sla_int',df.made_sla.cast('integer'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has multiple states(New, Active, Awaiting user info, Resolved, Closed etc. ) of an incident. With the help of the below command, we are just filtering one record per incident, that has the truly closed state of the incident. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_incidents=df.filter(\"incident_state=='Closed'\").sort(\"sys_mod_count\",ascending=False).dropDuplicates([\"number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the dependent and the independent variables that are identified as most useful attributes to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_unique_incidents.select([\n",
    "    'sys_mod_count',\n",
    "    'opened_by',\n",
    "    'location',\n",
    "    'category',\n",
    "    'priority',\n",
    "    'assignment_group',\n",
    "    'knowledge',\n",
    "    'resolved_duration',\n",
    "    'closed_duration',\n",
    "    'made_sla_int'\n",
    "    ]\n",
    ")\n",
    "\n",
    "data=data.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 70-30 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use StringIndexer to convert the categorical columns to hold numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_by_indexer = StringIndexer(inputCol='opened_by',outputCol='opened_by_index',handleInvalid='keep')\n",
    "location_indexer = StringIndexer(inputCol='location',outputCol='location_index',handleInvalid='keep')\n",
    "category_indexer = StringIndexer(inputCol='category',outputCol='category_index',handleInvalid='keep')\n",
    "priority_indexer = StringIndexer(inputCol='priority',outputCol='priority_index',handleInvalid='keep')\n",
    "assignment_group_indexer = StringIndexer(inputCol='assignment_group',outputCol='assignment_group_index',handleInvalid='keep')\n",
    "knowledge_indexer = StringIndexer(inputCol='knowledge',outputCol='knowledge_index',handleInvalid='keep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector assembler is used to create a vector of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'opened_by_index',\n",
    "        'location_index',\n",
    "        'category_index',\n",
    "        'priority_index',\n",
    "        'assignment_group_index',\n",
    "        'knowledge_index'\n",
    "    ],\n",
    "    outputCol=\"unscaled_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard scaler is used to scale the data for the linear SVC to perform well on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"unscaled_features\",outputCol=\"features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object for the Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = LinearSVC(labelCol='made_sla_int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data in the same way as that of the train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        opened_by_indexer,\n",
    "        location_indexer,\n",
    "        category_indexer,\n",
    "        priority_indexer,\n",
    "        assignment_group_indexer,\n",
    "        knowledge_indexer,\n",
    "        assembler,scaler,svc_model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sys_mod_count: integer (nullable = true)\n",
      " |-- opened_by: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- priority: string (nullable = true)\n",
      " |-- assignment_group: string (nullable = true)\n",
      " |-- knowledge: string (nullable = true)\n",
      " |-- resolved_duration: integer (nullable = true)\n",
      " |-- closed_duration: integer (nullable = true)\n",
      " |-- made_sla_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sys_mod_count: integer (nullable = true)\n",
      " |-- opened_by: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- priority: string (nullable = true)\n",
      " |-- assignment_group: string (nullable = true)\n",
      " |-- knowledge: string (nullable = true)\n",
      " |-- resolved_duration: integer (nullable = true)\n",
      " |-- closed_duration: integer (nullable = true)\n",
      " |-- made_sla_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "fit_model=pipe.fit(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sys_mod_count: int, opened_by: string, location: string, category: string, priority: string, assignment_group: string, knowledge: string, resolved_duration: int, closed_duration: int, made_sla_int: int, opened_by_index: double, location_index: double, category_index: double, priority_index: double, assignment_group_index: double, knowledge_index: double, unscaled_features: vector, features: vector, rawPrediction: vector, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = fit_model.transform(test_data)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|made_sla_int|prediction|\n",
      "+------------+----------+\n",
      "|           1|       1.0|\n",
      "|           0|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           0|       1.0|\n",
      "|           0|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           0|       1.0|\n",
      "|           0|       1.0|\n",
      "|           0|       1.0|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['made_sla_int','prediction']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the curve is 0.64\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderROC')\n",
    "AUC = AUC_evaluator.evaluate(results)\n",
    "\n",
    "print(f\"The area under the curve is {AUC:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A roughly 65% area under ROC denotes the model has performed reasonably well in predicting whether an incident has met the sla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Area under the PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the PR curve is 0.699293824000156\n"
     ]
    }
   ],
   "source": [
    "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderPR')\n",
    "PR = PR_evaluator.evaluate(results)\n",
    "\n",
    "print(\"The area under the PR curve is {}\".format(PR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.7144076233821647\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "ACC_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"made_sla_int\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = ACC_evaluator.evaluate(results)\n",
    "\n",
    "print(\"The accuracy of the model is {}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the confusion matrix: \n",
      " [[ 922 1743]\n",
      " [ 265 4101]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = results.select(\"made_sla_int\")\n",
    "y_true = y_true.toPandas()\n",
    " \n",
    "y_pred = results.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    " \n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Below is the confusion matrix: \\n {}\".format(cnf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cnf_matrix[0][0]\n",
    "fp = cnf_matrix[0][1]\n",
    "fn = cnf_matrix[1][0]\n",
    "tp = cnf_matrix[1][1]\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.70\n",
      "Recall: 0.94\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
