{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:16:03 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:16:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:16:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 PySpark Tutorials</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fba719d3b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+------+--------------------+\n",
      "|movieid|            title|       date|unkown|                 url|\n",
      "+-------+-----------------+-----------+------+--------------------+\n",
      "|      1| Toy Story (1995)|01-Jan-1995|  NULL|http://us.imdb.co...|\n",
      "|      2| GoldenEye (1995)|01-Jan-1995|  NULL|http://us.imdb.co...|\n",
      "|      3|Four Rooms (1995)|01-Jan-1995|  NULL|http://us.imdb.co...|\n",
      "|      4|Get Shorty (1995)|01-Jan-1995|  NULL|http://us.imdb.co...|\n",
      "|      5|   Copycat (1995)|01-Jan-1995|  NULL|http://us.imdb.co...|\n",
      "+-------+-----------------+-----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"movieid\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"unkown\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "movies = spark.read.csv('data/u.item', header=False, schema=schema,  sep = '|')\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies.createOrReplaceTempView(\"movies_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demonstration, note that when using sparksql, we can save the results in a temporary view\n",
    "# but this (and other sparksql switches) will not work with VSCode. It will work in Jupyter Notebook.\n",
    "# %%sparksql --view tempdf\n",
    "# select * from movies_tmp limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can use sparksql to show current tables, but this will only work in Jupyter Notebook. It will \n",
    "# not work in VSCode.\n",
    "#%%sparksql \n",
    "#SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieid|               title|\n",
      "+-------+--------------------+\n",
      "|      1|    Toy Story (1995)|\n",
      "|      2|    GoldenEye (1995)|\n",
      "|      3|   Four Rooms (1995)|\n",
      "|      4|   Get Shorty (1995)|\n",
      "|      5|      Copycat (1995)|\n",
      "|      6|Shanghai Triad (Y...|\n",
      "|      7|Twelve Monkeys (1...|\n",
      "|      8|         Babe (1995)|\n",
      "|      9|Dead Man Walking ...|\n",
      "|     10|  Richard III (1995)|\n",
      "|     11|Seven (Se7en) (1995)|\n",
      "|     12|Usual Suspects, T...|\n",
      "|     13|Mighty Aphrodite ...|\n",
      "|     14|  Postino, Il (1994)|\n",
      "|     15|Mr. Holland's Opu...|\n",
      "|     16|French Twist (Gaz...|\n",
      "|     17|From Dusk Till Da...|\n",
      "|     18|White Balloon, Th...|\n",
      "|     19|Antonia's Line (1...|\n",
      "|     20|Angels and Insect...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"\"\"SELECT\n",
    "  movieid,\n",
    "  title\n",
    "FROM movies_tmp\"\"\"\n",
    ")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:16:11 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/26 15:16:11 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/26 15:16:13 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/26 15:16:13 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore student@127.0.0.1\n",
      "23/10/26 15:16:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/10/26 15:16:15 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/10/26 15:16:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/10/26 15:16:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/26 15:16:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "spark_df.write.saveAsTable(\"movies\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userid|movieid|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|     3|881250949|\n",
      "|   186|    302|     3|891717742|\n",
      "|    22|    377|     1|878887116|\n",
      "|   244|     51|     2|880606923|\n",
      "|   166|    346|     1|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userid\", IntegerType(), True),\n",
    "    StructField(\"movieid\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "movierating = spark.read.csv('data/u.data', header=False, schema=schema,  sep = '\\t')\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "movierating.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movierating.write.saveAsTable(\"movieratings\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">userid</td><td style=\"font-weight: bold\">movieid</td><td style=\"font-weight: bold\">rating</td><td style=\"font-weight: bold\">timestamp</td></tr><tr><td>196</td><td>242</td><td>3</td><td>881250949</td></tr><tr><td>186</td><td>302</td><td>3</td><td>891717742</td></tr><tr><td>22</td><td>377</td><td>1</td><td>878887116</td></tr><tr><td>244</td><td>51</td><td>2</td><td>880606923</td></tr><tr><td>166</td><td>346</td><td>1</td><td>886397596</td></tr><tr><td>298</td><td>474</td><td>4</td><td>884182806</td></tr><tr><td>115</td><td>265</td><td>2</td><td>881171488</td></tr><tr><td>253</td><td>465</td><td>5</td><td>891628467</td></tr><tr><td>305</td><td>451</td><td>3</td><td>886324817</td></tr><tr><td>6</td><td>86</td><td>3</td><td>883603013</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from movieratings limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRating = spark.table('movieratings')\n",
    "dfMovies = spark.table('movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-------------+\n",
      "|userid|movieid|rating|timestamp|   prediction|\n",
      "+------+-------+------+---------+-------------+\n",
      "|   148|      1|     4|877019411|     5.108429|\n",
      "|   148|      8|     4|877020297|     5.034416|\n",
      "|   148|     50|     5|877016805|    4.3456655|\n",
      "|   148|     69|     5|877019101|     2.838674|\n",
      "|   148|     89|     5|877398587|    5.0523868|\n",
      "|   148|    151|     4|877400124|     4.194229|\n",
      "|   148|    181|     5|877399135|     4.172007|\n",
      "|   148|    191|     1|877020715|     3.891063|\n",
      "|   148|    194|     5|877015066|    4.3056793|\n",
      "|   148|    222|     4|877398901|    3.3137639|\n",
      "|   148|    228|     4|877016514|     6.162162|\n",
      "|   148|    432|     5|877019698|    3.6224227|\n",
      "|   148|    495|     4|877016735|    1.8733618|\n",
      "|   148|    549|     3|877398385|     4.830123|\n",
      "|   148|    596|     5|877020297|     5.160871|\n",
      "|   148|    663|     5|877399018|    4.3450947|\n",
      "|   148|   1012|     4|877400154|-0.0029671788|\n",
      "|   463|      1|     1|890453075|    2.9065013|\n",
      "|   463|      3|     2|877386083|    2.9016716|\n",
      "|   463|     13|     3|877385664|    2.9358096|\n",
      "+------+-------+------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for more on colaborative filtering, see here https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
    "# \n",
    "from pyspark.ml.recommendation import ALS\n",
    " \n",
    "#split training and testing\n",
    "(dftraining, dftest) = dfRating.randomSplit([0.8, 0.2])\n",
    " \n",
    "## Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userid\", \n",
    "    itemCol=\"movieid\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\")\n",
    "model = als.fit(dftraining)\n",
    " \n",
    "#display predicted rating\n",
    "predictions = model.transform(dftest)\n",
    "predictions.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
