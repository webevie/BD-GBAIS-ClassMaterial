{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Friends Data\n",
    "\n",
    "\n",
    "## Overview\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. DBFS is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in Python so the default cell type is Python. However, you can use different languages by using the %LANGUAGE syntax. Python, Scala, SQL, and R are all supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:15:25 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:15:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:15:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 PySpark Tutorials</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3faebbff50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----------+\n",
      "| id|    name|age|friendcount|\n",
      "+---+--------+---+-----------+\n",
      "|  0|    Will| 33|        385|\n",
      "|  1|Jean-Luc| 26|          2|\n",
      "|  2|    Hugh| 55|        221|\n",
      "|  3|  Deanna| 40|        465|\n",
      "|  4|   Quark| 68|         21|\n",
      "+---+--------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "# see here for more info on the schema: https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection\n",
    "# and here https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"friendcount\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "friends = spark.read.csv('data/fakefriends.csv', header=False, schema=schema)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "friends.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends.createOrReplaceTempView(\"fakefriends_csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running the cell below causes an error, you need to install the pyspark-magic module. Open a terminal (and make sure you have the spark conda environment active) and run the following command:\n",
    "```pip install sparksql-magic```\n",
    "\n",
    "NOTE: Remember, to activate the spark environment, run the following command:\n",
    "```conda activate spark```\n",
    "\n",
    "**NOTE2: You will need to restart your jupyter kernel after installing the module!!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">id</td><td style=\"font-weight: bold\">name</td><td style=\"font-weight: bold\">age</td><td style=\"font-weight: bold\">friendcount</td></tr><tr><td>0</td><td>Will</td><td>33</td><td>385</td></tr><tr><td>1</td><td>Jean-Luc</td><td>26</td><td>2</td></tr><tr><td>2</td><td>Hugh</td><td>55</td><td>221</td></tr><tr><td>3</td><td>Deanna</td><td>40</td><td>465</td></tr><tr><td>4</td><td>Quark</td><td>68</td><td>21</td></tr><tr><td>5</td><td>Weyoun</td><td>59</td><td>318</td></tr><tr><td>6</td><td>Gowron</td><td>37</td><td>220</td></tr><tr><td>7</td><td>Will</td><td>54</td><td>307</td></tr><tr><td>8</td><td>Jadzia</td><td>38</td><td>380</td></tr><tr><td>9</td><td>Hugh</td><td>27</td><td>181</td></tr><tr><td>10</td><td>Odo</td><td>53</td><td>191</td></tr><tr><td>11</td><td>Ben</td><td>57</td><td>372</td></tr><tr><td>12</td><td>Keiko</td><td>54</td><td>253</td></tr><tr><td>13</td><td>Jean-Luc</td><td>56</td><td>444</td></tr><tr><td>14</td><td>Hugh</td><td>43</td><td>49</td></tr><tr><td>15</td><td>Rom</td><td>36</td><td>49</td></tr><tr><td>16</td><td>Weyoun</td><td>22</td><td>323</td></tr><tr><td>17</td><td>Odo</td><td>35</td><td>13</td></tr><tr><td>18</td><td>Jean-Luc</td><td>45</td><td>455</td></tr><tr><td>19</td><td>Geordi</td><td>60</td><td>246</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from fakefriends_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">age</td><td style=\"font-weight: bold\">AvgFriendCount</td></tr><tr><td>31</td><td>267.3</td></tr><tr><td>65</td><td>298.2</td></tr><tr><td>53</td><td>222.9</td></tr><tr><td>34</td><td>245.5</td></tr><tr><td>28</td><td>209.1</td></tr><tr><td>26</td><td>242.1</td></tr><tr><td>27</td><td>228.1</td></tr><tr><td>44</td><td>282.2</td></tr><tr><td>22</td><td>206.4</td></tr><tr><td>47</td><td>233.2</td></tr><tr><td>52</td><td>340.6</td></tr><tr><td>40</td><td>250.8</td></tr><tr><td>20</td><td>165.0</td></tr><tr><td>57</td><td>258.8</td></tr><tr><td>54</td><td>278.1</td></tr><tr><td>48</td><td>281.4</td></tr><tr><td>19</td><td>213.3</td></tr><tr><td>64</td><td>281.3</td></tr><tr><td>41</td><td>268.6</td></tr><tr><td>43</td><td>230.6</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql \n",
    "select age, round(avg(friendcount), 1) As AvgFriendCount from fakefriends_csv group by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:15:34 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/26 15:15:34 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/26 15:15:36 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/26 15:15:36 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore student@127.0.0.1\n",
      "23/10/26 15:15:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/10/26 15:15:39 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/10/26 15:15:39 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/10/26 15:15:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/26 15:15:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# since our friends data is only stored in volatile memory, let's save the table into our spark-warehouse\n",
    "friends.write.saveAsTable(\"fake_friends\", mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
