{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:20:07 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.5.100 instead (on interface eth0)\n",
      "23/10/26 15:20:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 15:20:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ISM6562 PySpark Tutorials\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression to predict whether an incident met SLA\n",
    "\n",
    "The Incident Management dataset has about 141712 records of 24918 incidents. Each state of the incident is being captured as an individual record with few exceptions where the closed state of an incident is recorded more than once. With the help of the below segment of the code, we load and clean the Incident Management data so that only one record representing the truly closed state per incident is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 15:20:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|    number|incident_state|active|reassignment_count|reopen_count|sys_mod_count|made_sla|  caller_id|     opened_by|      opened_at|sys_created_by| sys_created_at|sys_updated_by|sys_updated_at|contact_type|    location|   category|    subcategory|  u_symptom|cmdb_ci|    impact|   urgency|    priority|assignment_group| assigned_to|knowledge|u_priority_confirmation|       notify|   problem_id|rfc|vendor|caused_by|closed_code|    resolved_by|    resolved_at|     closed_at|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "|INC0000045|        Closed| false|                 0|           0|            4|    true|Caller 2403|  Opened by  8|29/2/2016 01:16|  Created by 6|29/2/2016 01:23|Updated by 908|5/3/2016 12:00|       Phone|Location 143|Category 55|Subcategory 170| Symptom 72|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 56|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5|Resolved by 149|29/2/2016 11:29|5/3/2016 12:00|\n",
      "|INC0000047|        Closed| false|                 1|           0|            8|    true|Caller 2403|Opened by  397|29/2/2016 04:40|Created by 171|29/2/2016 04:57|Updated by 908|6/3/2016 10:00|       Phone|Location 165|Category 40|Subcategory 215|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 24| Resolver 89|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5| Resolved by 81|  1/3/2016 9:52|6/3/2016 10:00|\n",
      "|INC0000057|        Closed| false|                 0|           0|            6|    true|Caller 4416|  Opened by  8|29/2/2016 06:10|             ?|              ?|Updated by 908| 6/3/2016 3:00|       Phone|Location 204|Category 20|Subcategory 125|Symptom 471|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 70|  Resolver 6|     true|                  false|Do Not Notify|Problem ID  2|  ?|     ?|        ?|    code 10|  Resolved by 5|  1/3/2016 2:55| 6/3/2016 3:00|\n",
      "|INC0000060|        Closed| false|                 0|           0|            3|    true|Caller 4491|Opened by  180|29/2/2016 06:38| Created by 81|29/2/2016 06:42|Updated by 908|7/3/2016 13:00|       Phone|Location 204| Category 9| Subcategory 97|Symptom 450|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 25|Resolver 125|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 3|Resolved by 113| 2/3/2016 12:06|7/3/2016 13:00|\n",
      "|INC0000062|        Closed| false|                 1|           0|            7|   false|Caller 3765|Opened by  180|29/2/2016 06:58| Created by 81|29/2/2016 07:26|Updated by 908|5/3/2016 16:00|       Phone| Location 93|Category 53|Subcategory 168|Symptom 232|      ?|  1 - High|2 - Medium|    2 - High|        Group 23|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 7| Resolved by 62|29/2/2016 15:51|5/3/2016 16:00|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "df = spark.read.csv('data/incident_event_log_reduced.csv', header=True, inferSchema=True)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df=df.withColumn('resolved_ts',to_timestamp(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened_ts',to_timestamp(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('sys_created_ts',to_timestamp(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('sys_updated_ts',to_timestamp(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed_ts',to_timestamp(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('resolved',to_date(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened',to_date(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('sys_created',to_date(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('sys_updated',to_date(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed',to_date(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('knowledge', f.col('knowledge').cast('string')).\\\n",
    "        replace(['TRUE',], 'True', subset='knowledge').\\\n",
    "        replace(['FALSE'], 'False', subset='knowledge').\\\n",
    "        withColumn('resolved_duration',datediff(to_date(df.resolved_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('closed_duration',datediff(to_date(df.closed_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('made_sla_int',df.made_sla.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+----------+----------+-----------+-----------+----------+-----------------+---------------+------------+\n",
      "|    number|incident_state|active|reassignment_count|reopen_count|sys_mod_count|made_sla|  caller_id|     opened_by|      opened_at|sys_created_by| sys_created_at|sys_updated_by|sys_updated_at|contact_type|    location|   category|    subcategory|  u_symptom|cmdb_ci|    impact|   urgency|    priority|assignment_group| assigned_to|knowledge|u_priority_confirmation|       notify|   problem_id|rfc|vendor|caused_by|closed_code|    resolved_by|    resolved_at|     closed_at|        resolved_ts|          opened_ts|     sys_created_ts|     sys_updated_ts|          closed_ts|  resolved|    opened|sys_created|sys_updated|    closed|resolved_duration|closed_duration|made_sla_int|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+----------+----------+-----------+-----------+----------+-----------------+---------------+------------+\n",
      "|INC0000062|        Closed| false|                 1|           0|            7|   false|Caller 3765|Opened by  180|29/2/2016 06:58| Created by 81|29/2/2016 07:26|Updated by 908|5/3/2016 16:00|       Phone| Location 93|Category 53|Subcategory 168|Symptom 232|      ?|  1 - High|2 - Medium|    2 - High|        Group 23|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 7| Resolved by 62|29/2/2016 15:51|5/3/2016 16:00|2016-02-29 15:51:00|2016-02-29 06:58:00|2016-02-29 07:26:00|2016-03-05 16:00:00|2016-03-05 16:00:00|2016-02-29|2016-02-29| 2016-02-29| 2016-03-05|2016-03-05|                0|              5|           0|\n",
      "|INC0000066|        Closed| false|                 1|           0|            3|    true|Caller 3796| Opened by  24|29/2/2016 08:03| Created by 13|29/2/2016 08:18|Updated by 908|7/3/2016 15:00|       Phone|Location 161|Category 55|Subcategory 185|          ?|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 54|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 5|Resolved by 208| 2/3/2016 14:37|7/3/2016 15:00|2016-03-02 14:37:00|2016-02-29 08:03:00|2016-02-29 08:18:00|2016-03-07 15:00:00|2016-03-07 15:00:00|2016-03-02|2016-02-29| 2016-02-29| 2016-03-07|2016-03-07|                2|              7|           1|\n",
      "|INC0000079|        Closed| false|                 1|           0|            4|    true|Caller 2471|Opened by  180|29/2/2016 08:32|             ?|              ?|Updated by 908|5/3/2016 12:00|       Phone|Location 204|Category 55|Subcategory 170|Symptom 470|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 65|Resolver 180|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 6|Resolved by 163|29/2/2016 11:20|5/3/2016 12:00|2016-02-29 11:20:00|2016-02-29 08:32:00|               NULL|2016-03-05 12:00:00|2016-03-05 12:00:00|2016-02-29|2016-02-29|       NULL| 2016-03-05|2016-03-05|                0|              5|           1|\n",
      "|INC0000093|        Closed| false|                 0|           0|            1|    true|Caller 3545|Opened by  131|29/2/2016 08:56|             ?|              ?|Updated by 908|5/3/2016 10:00|       Phone| Location 93|Category 61|Subcategory 164|Symptom 580|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 70|           ?|     true|                  false|Do Not Notify|            ?|  ?|     ?|        ?|     code 7| Resolved by 66|              ?|5/3/2016 10:00|               NULL|2016-02-29 08:56:00|               NULL|2016-03-05 10:00:00|2016-03-05 10:00:00|      NULL|2016-02-29|       NULL| 2016-03-05|2016-03-05|             NULL|              5|           1|\n",
      "|INC0000111|        Closed| false|                 0|           0|            3|    true|Caller 4325|Opened by  180|29/2/2016 09:14| Created by 81|29/2/2016 09:19|Updated by 908|7/3/2016 15:00|       Phone| Location 93|Category 53|Subcategory 130|Symptom 470|      ?|2 - Medium|2 - Medium|3 - Moderate|        Group 54|           ?|     true|                  false|Do Not Notify|Problem ID  2|  ?|     ?|        ?|     code 7|Resolved by 208| 2/3/2016 14:56|7/3/2016 15:00|2016-03-02 14:56:00|2016-02-29 09:14:00|2016-02-29 09:19:00|2016-03-07 15:00:00|2016-03-07 15:00:00|2016-03-02|2016-02-29| 2016-02-29| 2016-03-07|2016-03-07|                2|              7|           1|\n",
      "+----------+--------------+------+------------------+------------+-------------+--------+-----------+--------------+---------------+--------------+---------------+--------------+--------------+------------+------------+-----------+---------------+-----------+-------+----------+----------+------------+----------------+------------+---------+-----------------------+-------------+-------------+---+------+---------+-----------+---------------+---------------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+----------+----------+-----------+-----------+----------+-----------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The data set has multiple states(New, Active, Awaiting user info, Resolved, Closed etc. ) of an incident. With the help \n",
    "# of the below command, we are just filtering one record per incident, that has the truly closed state of the incident. \n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "df_unique_incidents=df.filter(\"incident_state=='Closed'\").\\\n",
    "    sort(\"sys_mod_count\",ascending=False).\\\n",
    "    dropDuplicates([\"number\"])\n",
    "\n",
    "\n",
    "df_unique_incidents.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the dependent and the independent variables that are identified as most useful attributes to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_unique_incidents.select([\n",
    "    'sys_mod_count',\n",
    "    'opened_by',\n",
    "    'location',\n",
    "    'category',\n",
    "    'priority',\n",
    "    'assignment_group',\n",
    "    'knowledge',\n",
    "    'resolved_duration',\n",
    "    'closed_duration',\n",
    "    'made_sla_int'\n",
    "    ]\n",
    ")\n",
    "\n",
    "data=data.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 70-30 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    " \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer ,OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use StringIndexer to convert the categorical columns to hold numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_by_indexer = StringIndexer(inputCol='opened_by',outputCol='opened_by_index',handleInvalid='keep')\n",
    "location_indexer = StringIndexer(inputCol='location',outputCol='location_index',handleInvalid='keep')\n",
    "category_indexer = StringIndexer(inputCol='category',outputCol='category_index',handleInvalid='keep')\n",
    "priority_indexer = StringIndexer(inputCol='priority',outputCol='priority_index',handleInvalid='keep')\n",
    "assignment_group_indexer = StringIndexer(inputCol='assignment_group',outputCol='assignment_group_index',handleInvalid='keep')\n",
    "knowledge_indexer = StringIndexer(inputCol='knowledge',outputCol='knowledge_index',handleInvalid='keep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoderEstimator converts the indexed data into a vector which will be effectively handled by Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoder = OneHotEncoder(\n",
    "    inputCols=[\n",
    "        'opened_by_index',\n",
    "        'location_index',\n",
    "        'category_index',\n",
    "        'priority_index',\n",
    "        'assignment_group_index',\n",
    "        'knowledge_index'\n",
    "    ], \n",
    "    outputCols= [\n",
    "        'opened_by_vec',\n",
    "        'location_vec',\n",
    "        'category_vec',\n",
    "        'priority_vec',\n",
    "        'assignment_group_vec',\n",
    "        'knowledge_vec'],\n",
    "    handleInvalid='keep'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector assembler is used to create a vector of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"opened_by_vec\",\n",
    "        'location_vec',\n",
    "        'category_vec',\n",
    "        'priority_vec',\n",
    "        'assignment_group_vec',\n",
    "        'knowledge_vec'\n",
    "        ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object for the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(labelCol='made_sla_int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data in the same way as that of the train data. It also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        opened_by_indexer,\n",
    "        location_indexer,\n",
    "        category_indexer,\n",
    "        priority_indexer,\n",
    "        assignment_group_indexer,\n",
    "        knowledge_indexer,\n",
    "        data_encoder,\n",
    "        assembler,\n",
    "        lr_model\n",
    "    ]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run the pipeline\n",
    "fit_model=pipe.fit(train_data)\n",
    "\n",
    "# Store the results in a dataframe\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|made_sla_int|prediction|\n",
      "+------------+----------+\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           0|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           0|       0.0|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['made_sla_int','prediction']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderROC')\n",
    "\n",
    "AUC = AUC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the curve is 0.7281694544532962\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under the curve is {}\".format(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A roughly 73% area under ROC denotes the model has performed reasonably well in predicting whether an incident has met the sla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderPR')\n",
    "PR = PR_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the PR curve is 0.7610424679464421\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under the PR curve is {}\".format(PR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "ACC_evaluator = MulticlassClassificationEvaluator(  #  Multiclass or Binary, the accuracy is calculated in the same way.\n",
    "    labelCol=\"made_sla_int\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = ACC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.7578571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of the model is {}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "y_true = results.select(\"made_sla_int\")\n",
    "y_true = y_true.toPandas()\n",
    " \n",
    "y_pred = results.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    " \n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the confusion matrix \n",
      " [[1616 1088]\n",
      " [ 607 3689]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cnf_matrix[0][0]\n",
    "fp = cnf_matrix[0][1]\n",
    "fn = cnf_matrix[1][0]\n",
    "tp = cnf_matrix[1][1]\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.77\n",
      "Recall: 0.86\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
